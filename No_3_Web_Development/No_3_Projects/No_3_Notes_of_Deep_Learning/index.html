<!DOCTYPE html>
<html>
<!-- Title -->
<head>
    <title> Deep Learning </title>
    <link rel="stylesheet" href="css/style.css">
</head>
<!-- Main Fontend -->
<body>
    <!-- Heading -->
    <header>
        <h1> <i> Deep Learning </i> </h1>
    </header>
    <!-- Navigation Bar -->
    <nav>
        <a href="http://127.0.0.1:5500/No_3_Artificial_Intelligence_by_Python/No_1_Notes_of_Artificial_Intelligence/Artificial_Intelligence.html" target="_blank"> Artificial Intelligence </a> &nbsp; || &nbsp;
        <a href="http://127.0.0.1:5500/No_3_Artificial_Intelligence_by_Python/No_2_Machine_Learning/Notes_of_Machine_Learning/Machine_Learning.html" target="_blank"> Machine Learning </a> &nbsp; || &nbsp;
        <a href="https://www.kaggle.com/" target="_blank"> Resourses </a> &nbsp; || &nbsp;
        <a href="https://www.tensorflow.org/" target="_blank"> Tensorflow </a> &nbsp; || &nbsp;
        <a href="https://pytorch.org/" target="_blank"> Pytorch </a> &nbsp; || &nbsp;
        <a href="https://scikit-learn.org/stable/" target="_blank"> SK Learn </a> &nbsp; || &nbsp;
        <a href="https://keras.io/" target="_blank"> Keras </a> &nbsp; || &nbsp;
        <a href="https://www.ibm.com/topics/artificial-intelligence" target="_blank"> About AI </a>
    </nav>
    <!-- Main Content -->
    <main>
        <!-- Section For History of Deep Learning -->
        <section class="History">
            <h2> <i> About Deep Learning </i> </h2>
            <p>
                Deep learning is a subfield of machine learning that is inspired by the structure and function of the brain, known as artificial neural networks. 
                It is a type of artificial intelligence that involves training multi-layer neural networks on a large dataset, allowing the model to learn and 
                make intelligent decisions on its own.
                In deep learning, the model is composed of multiple layers of interconnected nodes, each of which performs a simple calculation. The layers work together 
                to produce a final output, with the output of one layer serving as input to the next. The model is trained by adjusting the weights and biases of the connections 
                between the nodes, so that the network can make accurate predictions for new, unseen data.
                Deep learning has been applied to a wide range of tasks, including image and speech recognition, natural language processing, and even game playing. It has also 
                been used for applications such as self-driving cars, recommender systems, and fraud detection.
                Overall, deep learning represents a major step forward in the development of artificial intelligence and has already had a significant impact on many industries.
            </p>
            <h2> <i> History of Deep Learning </i> </h2>
            <p class="ML_History">
                Deep learning is a subset of machine learning, which in turn is a subfield of artificial intelligence (AI). The history of deep learning can be traced back to the 1950s
                when artificial neural networks were first introduced, but it wasn't until the mid-2000s when advances in computing power and availability of large amounts of data enabled 
                the development of complex deep learning models. The first successful deep learning model was AlexNet in 2012, which outperformed traditional machine learning algorithms 
                on the image classification task in the ImageNet competition. Since then, deep learning has been applied to various fields such as computer vision, speech recognition,
                and natural language processing, and has achieved state-of-the-art performance in many tasks.
            </p>
            <h2 class="Revolution"> <i> Revolution of Deep Learning </i> </h2>
            <ul class="Revolution">
                <li> 1940s-1950s: The concept of artificial neural networks was first introduced. </li>
                <li> 1986: Backpropagation algorithm was developed, allowing for the efficient training of neural networks. </li>
                <li> 2012: AlexNet, a deep convolutional neural network, won the ImageNet competition, outperforming traditional machine learning algorithms. </li>
                <li> 2014: Google's DeepMind developed a deep reinforcement learning algorithm, called Deep Q-Network (DQN), which outperformed humans in several Atari games. </li>
                <li> 2015: Google's neural machine translation system, GNMT, demonstrated the ability to translate between multiple languages with remarkable accuracy. </li>
                <li> 2016: AlphaGo, developed by DeepMind, defeated the world champion in the game of Go, a game considered to be much more complex than chess. </li>
                <li> 2017: OpenAI's language model, GPT-2, was released, demonstrating remarkable language generation capabilities. </li>
                <li> 2018: Google's BERT, a deep bidirectional transformer, achieved state-of-the-art performance on several NLP tasks. </li>
                <li> 2019: NVIDIA's Megatron, a large transformer-based language model, was introduced, setting new records in language model performance. </li>
                <li> 2020: OpenAI's GPT-3, a significantly larger version of GPT-2, was released, demonstrating unprecedented language understanding and generation capabilities. </li>
            </ul>
            <ol class="DL_Topics">
                <h3> <i> Topics of Deep Learning </i> </h3>
                <li> Artificial Neural Networks </li>
                <li> Convolutional Neural Networks (CNN) </li>
                <li> Recurrent Neural Networks (RNN) </li>
                <li> Long Short-Term Memory (LSTM) </li>
                <li> Autoencoders </li>
                <li> Generative Adversarial Networks (GANs) </li>
                <li> Transfer Learning </li>
                <li> Deep Reinforcement Learning </li>
                <li> Computer Vision using Deep Learning </li>
                <li> Natural Language Processing using Deep Learning </li>
            </ol>
            <ol class="AI_Topics">
                <h3> <i> Topics of Artificial Intelligence </i> </h3>
                <li> Machine learning </li>
                <li> Deep learning </li>
                <li> Natural language processing </li>
                <li> Computer vision and image processing </li>
                <li> Robotics and control systems </li>
                <li> Neural networks </li>
                <li> Explainable AI (XAI) </li>
                <li> Chatbots and conversational agents </li>
                <li> Multi-agent systems </li>
                <li> Cognitive computing </li>
            </ol>
            <ol class="ML_Topics">
                <h3> <i> Topics of Machine Learning </i> </h3>
                <li> Supervised learning </li>
                <li> Unsupervised learning </li>
                <li> Semi-supervised learning </li>
                <li> Reinforcement learning </li>
                <li> Decision Trees </li>
                <li> Random Forests </li>
                <li> Support Vector Machines </li>
                <li> K-Nearest Neighbors </li>
                <li> Naive Bayes </li>
                <li> Linear Regression </li>
            </ol>
        </section>
        <!-- Section For Notes of Deep Learning -->
        <section class="Notes">
            <p> <h3 class="S_L_Perceptron"> Single Layer Perceptron</h3>
                A single layer perceptron is a type of artificial neural network that is designed to perform binary classification tasks. 
                It consists of a single layer of artificial neurons, also known as artificial perceptrons, which are connected to inputs 
                representing the features of the data and produce a single output that represents the prediction.
                Each artificial neuron in a single layer perceptron receives input from all the features of the data and applies a weight to each input. 
                The weighted inputs are then summed and passed through an activation function, which produces the output. The weights in a single layer 
                perceptron can be adjusted to minimize the error between the actual and predicted outputs during the training process.
            </p>
            <p> <h3 class="M_L_Perceptron"> Multi Layer Perceptron</h3>
                Multilayer Perceptron (MLP) is a type of artificial neural network that is used for supervised learning problems, such as classification or regression. 
                It is a feedforward network, meaning that data flows through the network in one direction, from input to output.
                An MLP consists of several layers of nodes, where each layer is fully connected to the next. The first layer is the input layer, which receives the input data. 
                The middle layers are called hidden layers and are used to learn complex representations of the data. The last layer is the output layer, which provides the 
                network's predictions.
            </p>
            <h2> Neural Network </h2>
            <p>
                A neural network is a type of machine learning model inspired by the structure and function of the human brain. Neural networks are designed to recognize 
                patterns in data and make predictions or decisions based on that data.
                In a neural network, simple processing nodes, called artificial neurons, are connected and organized into layers. The input data passes through the network, 
                and the neurons in each layer process the data and pass it on to the next layer. As the data flows through the network, the connections between the neurons 
                are adjusted to optimize the output of the network for a specific task, such as image classification or speech recognition. This process is called training 
                the neural network.
            </p>
            <p> <h3 class="RNN"> Recurrent Neural Network </h3>
                Recurrent Neural Networks (RNNs) are a type of artificial neural network specifically designed to handle sequential data. Unlike feedforward neural networks, 
                which take a fixed-sized input and produce a fixed-sized output, RNNs take a variable-length input and produce a variable-length output.
            </p>
            <p> <h3 class="CNN"> Convolutional Neural Network </h3>
                Convolutional Neural Networks (CNNs) are a type of artificial neural network specifically designed for image recognition tasks. They are called "convolutional" 
                because they use a mathematical operation called convolution, which allows the network to learn and identify features in the image.
            </p>
            <h2> Text Mining</h2>
            <p>
                Text Mining is the process of deriving high quality information from the text.
                The overall goal is to turn the text into data for analysis, via application of Natural Language Processing (NLP).
            </p>
            <h2> Natural Language Processing </h2>
            <p>
                Natural Language Processing (NLP) is a subfield of artificial intelligence that deals with the interactions between computers and humans in natural language. 
                NLP's aim is to enable computers to understand, interpret, and generate human language in a way that resembles how people communicate. This involves a range 
                of tasks, such as text classification, information extraction, machine translation, sentiment analysis, and question answering, among others. To achieve these 
                tasks, NLP combines techniques from linguistics, computer science, and mathematics to process and analyze large amounts of human language data. The goal of NLP 
                is to create systems that can perform human-like language tasks and interact with users in a more natural and intuitive way.
            </p>
            <ul> <h3> Terminologies of Natural Language Processing </h3> 
                <li> Tokenization :- The process of splitting the whole data into small chunks is known as tokenization. </li>
                <li> Stemming :- Normalize words into its base form or root form. </li>
                <li> Lemmitization :- Understanding grammer give the original word. </li>
                <li> Stop Words :- Avoid the non weighted words. </li>
                <li>
                    Document Term Matrix :- Mathematical representation of a collection of documents, typically used in natural language processing (NLP). It is a table where 
                    each row represents a document and each column represents a unique term in the corpus. The values in the table indicate the frequency or weight of each term 
                    in each document. The DTM is often used as the input for text mining and information retrieval tasks, such as topic modeling and sentiment analysis. 
                </li>
            </ul>
        </section>
    </main>
    <!-- Ending -->
    <footer>
        <!--  -->
    </footer>
</body>
</html>